{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1fa78774-8c5d-4a82-aff3-e6940da4b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d39e4dd0-fdf7-45b8-b556-c7e414fd642f",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_df = pd.read_csv('IMDB Dataset.csv')\n",
    "df = temp_df.iloc[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c45304-9171-4af7-b2a0-583b010ef161",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3ddf3b1-cb85-4c09-8836-a921730be1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment\n",
       "positive    5028\n",
       "negative    4972\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "420f48b6-6d7c-406e-800f-f859dbf8e454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5a4ef89-4739-4b24-945e-8fc144ad8ffb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "382a2d4e-6150-4af6-8914-770ee43abf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6540\\3424306917.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df.drop_duplicates(inplace = True)\n"
     ]
    }
   ],
   "source": [
    "df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6d39481-a1aa-430b-9ccb-45e8c3e64f0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5c9d74eb-b2e1-4b05-a879-be339dd6439f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Preprocessing\n",
    "# Remove tags\n",
    "# lowercase\n",
    "# remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56454911-6849-450e-9d8a-a8182acbfb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def remove_tags(raw_text):\n",
    "    cleaned_text = re.sub(re.compile('<.*?>'), '', raw_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f06603d0-3c06-4cd2-84ba-f029cac9a351",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6540\\2336150696.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review'] = df['review'].apply(remove_tags)\n"
     ]
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a00d57f-2a5d-4f18-a3ad-64242ac5a27a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6540\\740760900.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review'] = df['review'].apply(lambda x:x.lower())\n"
     ]
    }
   ],
   "source": [
    "df['review'] = df['review'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71a1dde3-03ec-4ff6-b2fe-7efa7ccb0539",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_6540\\2826946130.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['review'] = df['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_list = stopwords.words('english')\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e027725d-7962-42c9-ae5e-bb3f942e92ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production. filming technique...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's family little boy (jake) thi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love time money\" visually stu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>fun, entertaining movie wwii german spy (julie...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>give break. anyone say \"good hockey movie\"? kn...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>movie bad movie. watching endless series bad h...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>movie probably made entertain middle school, e...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>smashing film film-making. shows intense stran...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9983 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review sentiment\n",
       "0     one reviewers mentioned watching 1 oz episode ...  positive\n",
       "1     wonderful little production. filming technique...  positive\n",
       "2     thought wonderful way spend time hot summer we...  positive\n",
       "3     basically there's family little boy (jake) thi...  negative\n",
       "4     petter mattei's \"love time money\" visually stu...  positive\n",
       "...                                                 ...       ...\n",
       "9995  fun, entertaining movie wwii german spy (julie...  positive\n",
       "9996  give break. anyone say \"good hockey movie\"? kn...  negative\n",
       "9997  movie bad movie. watching endless series bad h...  negative\n",
       "9998  movie probably made entertain middle school, e...  negative\n",
       "9999  smashing film film-making. shows intense stran...  positive\n",
       "\n",
       "[9983 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c88501c9-2c45-4fdd-85f4-cf89e80e76dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0:1]\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7a11facc-00cd-4654-bdf2-122aa1e8760e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production. filming technique...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's family little boy (jake) thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love time money\" visually stu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>fun, entertaining movie wwii german spy (julie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>give break. anyone say \"good hockey movie\"? kn...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>movie bad movie. watching endless series bad h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>movie probably made entertain middle school, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>smashing film film-making. shows intense stran...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9983 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 review\n",
       "0     one reviewers mentioned watching 1 oz episode ...\n",
       "1     wonderful little production. filming technique...\n",
       "2     thought wonderful way spend time hot summer we...\n",
       "3     basically there's family little boy (jake) thi...\n",
       "4     petter mattei's \"love time money\" visually stu...\n",
       "...                                                 ...\n",
       "9995  fun, entertaining movie wwii german spy (julie...\n",
       "9996  give break. anyone say \"good hockey movie\"? kn...\n",
       "9997  movie bad movie. watching endless series bad h...\n",
       "9998  movie probably made entertain middle school, e...\n",
       "9999  smashing film film-making. shows intense stran...\n",
       "\n",
       "[9983 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e7f51b6-39dd-4a80-82de-c0b43f49af92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       positive\n",
       "1       positive\n",
       "2       positive\n",
       "3       negative\n",
       "4       positive\n",
       "          ...   \n",
       "9995    positive\n",
       "9996    negative\n",
       "9997    negative\n",
       "9998    negative\n",
       "9999    positive\n",
       "Name: sentiment, Length: 9983, dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a15301d-a5d9-4589-a6e5-dfa14246d76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6743bdf3-cf86-4d46-af75-a0e47e5601c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee7182c2-83e1-45c7-9271-1722df43b169",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7986, 1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "279d65e3-2592-44b9-bdbc-5811095351c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "50d33551-ed72-4b00-8406-35e889b78c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "602b531a-5518-4a4e-8485-97394fc08e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28dcf208-4920-4964-adc5-5e1379baf1e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7986, 48282)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b3a96a76-bf5f-49e6-bf62-b0164cdee1db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train_bow, y_train)\n",
    "y_pred = mnb.predict(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5604bcd-a211-4270-aa2d-99586a18fbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8487731597396094"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d61e1cf1-2a3f-4382-8309-f5c415377250",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[834, 118],\n",
       "       [184, 861]], dtype=int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ff784d89-a267-48ff-b2bf-32986e6a042a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8452679018527791"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e5cc73fd-f094-4fd4-ba9f-2582535901d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8347521281922884"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(max_features=3000)\n",
    "\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99adfa6d-cdd6-4fd9-bfd1-b4b4ab767aba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8467701552328493"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),max_features=5000)\n",
    "\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f72a678-8a09-4fac-a429-c09dfd2cc7e7",
   "metadata": {},
   "source": [
    "# Using TfIdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38a6e173-aea9-4c53-966b-a0caf303014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a328950-7bdc-46ec-9684-8c779cbb00de",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9f19e725-bc38-4aa8-9752-16eeacca3161",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train['review']).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5df0dcbe-f13e-4f12-9556-ce01d9f2bba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8467701552328493"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b55fcd85-62eb-4227-aaa1-e2130c6c843f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "58c3d15f-c4ac-410b-a932-7ec4b46e7ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec,KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5fcb6e31-1c29-410e-8a63-5cf7414fbd92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "model = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18e34e4b-7c2c-441d-b62d-2d3aa230d49b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model['cricket'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d4aa89cb-be28-4f67-a961-57e3bb6ff330",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f0b705e-4796-4813-b1b4-8e84652059a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " \"he'd\",\n",
       " \"he'll\",\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " \"he's\",\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " \"i'd\",\n",
       " 'if',\n",
       " \"i'll\",\n",
       " \"i'm\",\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it'd\",\n",
       " \"it'll\",\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " \"i've\",\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she'd\",\n",
       " \"she'll\",\n",
       " \"she's\",\n",
       " 'should',\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " \"should've\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " \"they'd\",\n",
       " \"they'll\",\n",
       " \"they're\",\n",
       " \"they've\",\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " \"we'd\",\n",
       " \"we'll\",\n",
       " \"we're\",\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " \"we've\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " 'your',\n",
       " \"you're\",\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " \"you've\"]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c77961a4-c179-4399-b67d-0fec274a05df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "\n",
    "X_train = X_train['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))\n",
    "# Remove stopwords\n",
    "\n",
    "X_test = X_test['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "707c7e67-d2d5-41a7-abc3-69702b63e220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl.metadata (28 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl.metadata (2.2 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl.metadata (8.8 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
      "  Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (4.66.5)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (1.26.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (2.8.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (3.1.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (75.1.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
      "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading blis-1.3.0-cp312-cp312-win_amd64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
      "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Downloading numpy-2.3.4-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from jinja2->spacy) (2.1.3)\n",
      "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.3.1-cp312-cp312-win_amd64.whl.metadata (10 kB)\n",
      "Downloading spacy-3.8.7-cp312-cp312-win_amd64.whl (13.9 MB)\n",
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.3.4 which is incompatible.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.3.4 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.3.4 which is incompatible.\n",
      "scipy 1.13.1 requires numpy<2.3,>=1.22.4, but you have numpy 2.3.4 which is incompatible.\n",
      "streamlit 1.37.1 requires protobuf<6,>=3.20, but you have protobuf 6.32.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ---------------------------------------- 0.0/13.9 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/13.9 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/13.9 MB 1.5 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.3/13.9 MB 1.6 MB/s eta 0:00:08\n",
      "   ---- ----------------------------------- 1.6/13.9 MB 1.6 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.1/13.9 MB 1.7 MB/s eta 0:00:07\n",
      "   ------- -------------------------------- 2.6/13.9 MB 1.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.1/13.9 MB 1.9 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 3.7/13.9 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 4.2/13.9 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 4.7/13.9 MB 2.1 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 5.5/13.9 MB 2.2 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 6.0/13.9 MB 2.3 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.8/13.9 MB 2.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 7.3/13.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 8.1/13.9 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 8.7/13.9 MB 2.5 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.9/13.9 MB 2.4 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 10.0/13.9 MB 2.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 10.7/13.9 MB 2.5 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 11.5/13.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.3/13.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 12.8/13.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.9 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.9/13.9 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading cymem-2.0.11-cp312-cp312-win_amd64.whl (39 kB)\n",
      "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
      "Downloading murmurhash-1.0.13-cp312-cp312-win_amd64.whl (24 kB)\n",
      "Downloading preshed-3.0.10-cp312-cp312-win_amd64.whl (116 kB)\n",
      "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Downloading srsly-2.5.1-cp312-cp312-win_amd64.whl (632 kB)\n",
      "   ---------------------------------------- 0.0/632.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 632.6/632.6 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading thinc-8.3.6-cp312-cp312-win_amd64.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ----------------- ---------------------- 0.8/1.7 MB 4.2 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.3/1.7 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading numpy-2.3.4-cp312-cp312-win_amd64.whl (12.8 MB)\n",
      "   ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.8 MB 4.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/12.8 MB 3.5 MB/s eta 0:00:04\n",
      "   ---- ----------------------------------- 1.6/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "   -------- ------------------------------- 2.6/12.8 MB 3.2 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 3.1/12.8 MB 2.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 3.7/12.8 MB 2.7 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 3.9/12.8 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------- -------------------------- 4.5/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 4.7/12.8 MB 2.5 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "   ---------------- ----------------------- 5.2/12.8 MB 2.3 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 5.5/12.8 MB 2.2 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 5.8/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 6.0/12.8 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 6.3/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 6.6/12.8 MB 2.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 6.8/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 7.1/12.8 MB 1.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 7.6/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 7.9/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 8.1/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 8.4/12.8 MB 1.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.7/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.9/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 8.9/12.8 MB 1.7 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 9.4/12.8 MB 1.6 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 9.7/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.0/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 10.2/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 10.5/12.8 MB 1.6 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 10.7/12.8 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 11.0/12.8 MB 1.5 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.3/12.8 MB 1.5 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.5/12.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.5/12.8 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 11.8/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 12.1/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.3/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.6/12.8 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.8/12.8 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Downloading blis-1.3.0-cp312-cp312-win_amd64.whl (6.3 MB)\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/6.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/6.3 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/6.3 MB 989.2 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 960.2 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/6.3 MB 960.2 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.0/6.3 MB 968.5 kB/s eta 0:00:06\n",
      "   ------ --------------------------------- 1.0/6.3 MB 968.5 kB/s eta 0:00:06\n",
      "   -------- ------------------------------- 1.3/6.3 MB 871.6 kB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 1.6/6.3 MB 856.1 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 846.1 kB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 1.8/6.3 MB 846.1 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.1/6.3 MB 815.6 kB/s eta 0:00:06\n",
      "   ------------- -------------------------- 2.1/6.3 MB 815.6 kB/s eta 0:00:06\n",
      "   --------------- ------------------------ 2.4/6.3 MB 794.4 kB/s eta 0:00:05\n",
      "   --------------- ------------------------ 2.4/6.3 MB 794.4 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 782.5 kB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 2.6/6.3 MB 782.5 kB/s eta 0:00:05\n",
      "   ------------------ --------------------- 2.9/6.3 MB 769.7 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.3 MB 769.0 kB/s eta 0:00:05\n",
      "   -------------------- ------------------- 3.1/6.3 MB 769.0 kB/s eta 0:00:05\n",
      "   --------------------- ------------------ 3.4/6.3 MB 765.6 kB/s eta 0:00:04\n",
      "   --------------------- ------------------ 3.4/6.3 MB 765.6 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 762.7 kB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 3.7/6.3 MB 762.7 kB/s eta 0:00:04\n",
      "   ------------------------- -------------- 3.9/6.3 MB 760.2 kB/s eta 0:00:04\n",
      "   -------------------------- ------------- 4.2/6.3 MB 755.7 kB/s eta 0:00:03\n",
      "   -------------------------- ------------- 4.2/6.3 MB 755.7 kB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 4.5/6.3 MB 758.3 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 758.5 kB/s eta 0:00:03\n",
      "   ------------------------------ --------- 4.7/6.3 MB 758.5 kB/s eta 0:00:03\n",
      "   ------------------------------- -------- 5.0/6.3 MB 760.7 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.3 MB 760.9 kB/s eta 0:00:02\n",
      "   --------------------------------- ------ 5.2/6.3 MB 760.9 kB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 760.9 kB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 5.5/6.3 MB 760.9 kB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.8/6.3 MB 762.7 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 765.8 kB/s eta 0:00:01\n",
      "   -------------------------------------- - 6.0/6.3 MB 765.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 6.3/6.3 MB 763.2 kB/s eta 0:00:00\n",
      "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
      "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/5.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.3/5.4 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.5/5.4 MB 885.6 kB/s eta 0:00:06\n",
      "   --- ------------------------------------ 0.5/5.4 MB 885.6 kB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 0.8/5.4 MB 907.1 kB/s eta 0:00:06\n",
      "   ------- -------------------------------- 1.0/5.4 MB 949.8 kB/s eta 0:00:05\n",
      "   --------- ------------------------------ 1.3/5.4 MB 945.5 kB/s eta 0:00:05\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 976.0 kB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 1.6/5.4 MB 976.0 kB/s eta 0:00:04\n",
      "   ------------- -------------------------- 1.8/5.4 MB 987.4 kB/s eta 0:00:04\n",
      "   --------------- ------------------------ 2.1/5.4 MB 1.0 MB/s eta 0:00:04\n",
      "   ----------------- ---------------------- 2.4/5.4 MB 1.0 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.9/5.4 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 2.9/5.4 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 3.1/5.4 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 3.7/5.4 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 3.9/5.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 4.2/5.4 MB 1.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 4.7/5.4 MB 1.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 5.0/5.4 MB 1.3 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 5.2/5.4 MB 1.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 5.4/5.4 MB 1.2 MB/s eta 0:00:00\n",
      "Downloading marisa_trie-1.3.1-cp312-cp312-win_amd64.whl (138 kB)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, numpy, murmurhash, marisa-trie, cloudpathlib, catalogue, srsly, preshed, language-data, blis, langcodes, confection, weasel, thinc, spacy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "Successfully installed blis-1.3.0 catalogue-2.0.10 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.11 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 murmurhash-1.0.13 numpy-2.3.4 preshed-3.0.10 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 wasabi-1.1.3 weasel-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "08c30874-0bb9-4cb7-8090-2082257ca6c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01men_core_web_sm\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Load the spacy model. This takes a few seconds.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py:6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, Dict, Iterable, Union\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# set library-specific custom warning handling before doing anything else\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01merrors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m setup_default_warnings\n\u001b[0;32m      8\u001b[0m setup_default_warnings()  \u001b[38;5;66;03m# noqa: E402\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# These are imported as part of the API\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\errors.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Literal\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mErrorsWithCodes\u001b[39;00m(\u001b[38;5;28mtype\u001b[39m):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getattribute__\u001b[39m(\u001b[38;5;28mself\u001b[39m, code):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\spacy\\compat.py:39\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcatalogue\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _importlib_metadata \u001b[38;5;28;01mas\u001b[39;00m importlib_metadata  \u001b[38;5;66;03m# type: ignore[no-redef]    # noqa: F401\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthinc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Optimizer  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     41\u001b[0m pickle \u001b[38;5;241m=\u001b[39m pickle\n\u001b[0;32m     42\u001b[0m copy_reg \u001b[38;5;241m=\u001b[39m copy_reg\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\api.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackends\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CupyOps,\n\u001b[0;32m      3\u001b[0m     MPSOps,\n\u001b[0;32m      4\u001b[0m     NumpyOps,\n\u001b[0;32m      5\u001b[0m     Ops,\n\u001b[0;32m      6\u001b[0m     get_current_ops,\n\u001b[0;32m      7\u001b[0m     get_ops,\n\u001b[0;32m      8\u001b[0m     set_current_ops,\n\u001b[0;32m      9\u001b[0m     set_gpu_allocator,\n\u001b[0;32m     10\u001b[0m     use_ops,\n\u001b[0;32m     11\u001b[0m     use_pytorch_for_gpu_memory,\n\u001b[0;32m     12\u001b[0m     use_tensorflow_for_gpu_memory,\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompat\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_mxnet, enable_tensorflow, has_cupy\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, ConfigValidationError, registry\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\__init__.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_cupy_allocators\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cupy_pytorch_allocator, cupy_tensorflow_allocator\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_server\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ParamServer\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcupy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CupyOps\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmps_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MPSOps\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\cupy_ops.py:16\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      7\u001b[0m     is_cupy_array,\n\u001b[0;32m      8\u001b[0m     is_mxnet_gpu_array,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     torch2xp,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _custom_kernels\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnumpy_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m NumpyOps\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Ops\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@registry\u001b[39m\u001b[38;5;241m.\u001b[39mops(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCupyOps\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mCupyOps\u001b[39;00m(Ops):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\thinc\\backends\\numpy_ops.pyx:1\u001b[0m, in \u001b[0;36minit thinc.backends.numpy_ops\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: numpy.core.multiarray failed to import (auto-generated because you didn't call 'numpy.import_array()' after cimporting numpy; use '<void>numpy._import_array' to disable if you are certain you don't need it)."
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "# Load the spacy model. This takes a few seconds.\n",
    "nlp = en_core_web_sm.load()\n",
    "# Process a sentence using the model\n",
    "doc = nlp(X_train.values[0])\n",
    "print(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66ca509d-6e3a-4e52-a6c1-ba3f93418d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  ERROR: HTTP error 404 while getting https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz\n",
      "ERROR: Could not install requirement https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz because of HTTP error 404 Client Error: Not Found for url: https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz for URL https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.0/en_core_web_sm-2.2.5.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79364066-d895-4a7b-a4c3-a4d6a3d7e155",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 148, in _get_module_details\n",
      "  File \"<frozen runpy>\", line 112, in _get_module_details\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\spacy\\__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\spacy\\errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\spacy\\compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\thinc\\__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\thinc\\config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\thinc\\types.py\", line 27, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\thinc\\compat.py\", line 99, in <module>\n",
      "    import h5py\n",
      "  File \"C:\\Users\\HP\\anaconda3\\Lib\\site-packages\\h5py\\__init__.py\", line 45, in <module>\n",
      "    from ._conv import register_converters as _register_converters, \\\n",
      "  File \"h5py\\\\_conv.pyx\", line 1, in init h5py._conv\n",
      "  File \"h5py\\\\h5r.pyx\", line 1, in init h5py.h5r\n",
      "  File \"h5py\\\\h5p.pyx\", line 1, in init h5py.h5p\n",
      "ValueError: numpy.dtype size changed, may indicate binary incompatibility. Expected 96 from C header, got 88 from PyObject\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bacb19b7-de24-43d8-b2e6-96d258bb0636",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_arr \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m----> 3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(item)\n\u001b[0;32m      4\u001b[0m     input_arr\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mvector)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "input_arr = []\n",
    "for item in X_train.values:\n",
    "    doc = nlp(item)\n",
    "    input_arr.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d5bbed40-b533-4fe1-a2ff-7faca0eacd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_arr = np.array(input_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "140ebefa-04ca-45cf-9174-c1a4e9cccef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "50e1dda0-f5a6-4bbc-908c-e2219a785202",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m input_test_arr \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m X_test\u001b[38;5;241m.\u001b[39mvalues:\n\u001b[1;32m----> 3\u001b[0m     doc \u001b[38;5;241m=\u001b[39m nlp(item)\n\u001b[0;32m      4\u001b[0m     input_test_arr\u001b[38;5;241m.\u001b[39mappend(doc\u001b[38;5;241m.\u001b[39mvector)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "input_test_arr = []\n",
    "for item in X_test.values:\n",
    "    doc = nlp(item)\n",
    "    input_test_arr.append(doc.vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50759e9-81fd-4af1-9cd9-eea1fbc2dd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_arr = np.array(input_test_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcac9ef-ea50-4aa1-8b68-97131f7304b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1d555-d517-4373-be01-f666b2c97178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d3e26dc-3831-4bbe-b01a-56bceec34dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(input_arr,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5b7f5b-6b94-427d-b69c-93439920d9dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = gnb.predict(input_test_arr)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
