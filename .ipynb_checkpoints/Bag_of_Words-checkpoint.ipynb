{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Bag of Words"
      ],
      "metadata": {
        "id": "sVJIyO9JJCIy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40db0475"
      },
      "source": [
        "# Replace this list with your text data\n",
        "text_data = [\n",
        "    \"This is the first document.\",\n",
        "    \"This document is the second document.\",\n",
        "    \"And this is the third one.\",\n",
        "    \"Is this the first document?\"\n",
        "]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9b252408",
        "outputId": "601744a1-c250-436c-804f-d1822a6f2449"
      },
      "source": [
        "# Display the bag-of-words matrix\n",
        "print(\"Bag-of-Words Matrix:\")\n",
        "print(bag_of_words_matrix.toarray())"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Matrix:\n",
            "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
            " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
            " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9740962",
        "outputId": "26c6c64a-4154-4c00-acf6-c26f30e016e9"
      },
      "source": [
        "# Display the feature names\n",
        "print(\"\\nFeature Names:\")\n",
        "print(feature_names)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Feature Names:\n",
            "['and' 'and this' 'document' 'document is' 'first' 'first document' 'is'\n",
            " 'is the' 'is this' 'one' 'second' 'second document' 'the' 'the first'\n",
            " 'the second' 'the third' 'third' 'third one' 'this' 'this document'\n",
            " 'this is' 'this the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e56235b",
        "outputId": "18150b2f-9627-4480-b19a-1781f895f645"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Create a CountVectorizer object with ngram_range set to (1, 2) for unigrams and bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "\n",
        "# Fit and transform the text data\n",
        "bag_of_words_matrix = vectorizer.fit_transform(text_data)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display the bag-of-words matrix and feature names\n",
        "print(\"Bag-of-Words Matrix:\")\n",
        "print(bag_of_words_matrix.toarray())\n",
        "print(\"\\nFeature Names:\")\n",
        "print(feature_names)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bag-of-Words Matrix:\n",
            "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
            " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
            " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n",
            "\n",
            "Feature Names:\n",
            "['and' 'and this' 'document' 'document is' 'first' 'first document' 'is'\n",
            " 'is the' 'is this' 'one' 'second' 'second document' 'the' 'the first'\n",
            " 'the second' 'the third' 'third' 'third one' 'this' 'this document'\n",
            " 'this is' 'this the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer.vocabulary_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GjTLUwpbD7mE",
        "outputId": "6c9ef46f-e95b-4f25-8085-dc75a5df4e8b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'this': 8,\n",
              " 'is': 3,\n",
              " 'the': 6,\n",
              " 'first': 2,\n",
              " 'document': 1,\n",
              " 'second': 5,\n",
              " 'and': 0,\n",
              " 'third': 7,\n",
              " 'one': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "bag_of_words_matrix.toarray()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yrBkTJVEB_s",
        "outputId": "736e4097-e955-4e1d-a5a0-3e5161f32639"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0],\n",
              "       [0, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0],\n",
              "       [1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0],\n",
              "       [0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8811c4c"
      },
      "source": [
        "Here are some of the key hyperparameters of `CountVectorizer` and their descriptions:\n",
        "\n",
        "*   **`input`**: Type of the input. Can be 'filename', 'file' or 'content'. Default is 'content'.\n",
        "*   **`encoding`**: If bytes or files are given to analyze, this is the encoding used to decode. Default is 'utf-8'.\n",
        "*   **`decode_error`**: Instruction on what to do if a byte sequence is given to analyze that cannot be decoded using `encoding`. 'strict' raises a UnicodeDecodeError. 'ignore' ignores errors. 'replace' replaces errors with a replacement character. Default is 'strict'.\n",
        "*   **`strip_accents`**: Remove accents and perform other character normalization during the preprocessing step. Can be 'ascii', 'unicode', or None. Default is None.\n",
        "*   **`lowercase`**: Convert all characters to lowercase before tokenizing. Default is True.\n",
        "*   **`preprocessor`**: Override the preprocessing (strip_accents and lowercase) stage by providing a callable that takes an entire document as input and returns a transformed document. Default is None.\n",
        "*   **`tokenizer`**: Override the string tokenization step by providing a callable. Default is None (uses the default word tokenizer).\n",
        "*   **`stop_words`**: Remove stop words. Can be 'english', a list of strings, or None. Default is None.\n",
        "*   **`token_pattern`**: Regular expression denoting what constitutes a \"token\". Default is `r'(?u)\\b\\w\\w+\\b'`.\n",
        "*   **`ngram_range`**: The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that `min_n <= n <= max_n` will be used. Default is `(1, 1)` (only unigrams).\n",
        "*   **`analyzer`**: Whether the feature should be made of word or character n-grams. Can be 'word' or 'char'. Default is 'word'.\n",
        "*   **`max_df`**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). Can be a float in the range [0.0, 1.0] or an integer. Default is 1.0.\n",
        "*   **`min_df`**: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. Can be a float in the range [0.0, 1.0] or an integer. Default is 1.\n",
        "*   **`max_features`**: If not None, build a vocabulary that only considers the top `max_features` ordered by term frequency across the corpus. Default is None.\n",
        "*   **`vocabulary`**: Either a mapping where keys are terms and values are indices in the feature matrix, or an iterable of terms. If not given, a vocabulary is built from the input documents. Default is None.\n",
        "*   **`binary`**: If True, all non-zero counts are set to 1. This is good for discrete probabilistic models that model binary events. Default is False.\n",
        "*   **`dtype`**: Type of the matrix returned by fit_transform() or transform(). Default is `numpy.int64`."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TF-IDF"
      ],
      "metadata": {
        "id": "IES5dpwnQizV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Replace this with your new text data\n",
        "new_text_data = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"The dog is lazy.\",\n",
        "    \"The fox is quick and brown.\"\n",
        "]\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the new text data\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(new_text_data)\n",
        "\n",
        "# Get the feature names (words)\n",
        "new_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Display the TF-IDF matrix and feature names\n",
        "print(\"TF-IDF Matrix:\")\n",
        "print(tfidf_matrix.toarray())\n",
        "print(\"\\nFeature Names:\")\n",
        "print(new_feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c95DYxruPhj9",
        "outputId": "5114ec75-367d-48ca-c8a9-f70cf74b1bac"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix:\n",
            "[[0.         0.30330642 0.30330642 0.30330642 0.         0.3988115\n",
            "  0.30330642 0.3988115  0.30330642 0.47108899]\n",
            " [0.         0.         0.52682017 0.         0.52682017 0.\n",
            "  0.52682017 0.         0.         0.40912286]\n",
            " [0.52253528 0.39740155 0.         0.39740155 0.39740155 0.\n",
            "  0.         0.         0.39740155 0.30861775]]\n",
            "\n",
            "Feature Names:\n",
            "['and' 'brown' 'dog' 'fox' 'is' 'jumps' 'lazy' 'over' 'quick' 'the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b051d68"
      },
      "source": [
        "### Comparing CountVectorizer and TfidfVectorizer Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "171b45c7",
        "outputId": "6d1dd767-fe59-4c25-bfca-c0fadfcd0f42"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create a TfidfVectorizer object\n",
        "tfidf_vectorizer_compare = TfidfVectorizer(ngram_range=(1, 2)) # Using the same ngram_range as CountVectorizer for comparison\n",
        "\n",
        "# Fit and transform the original text data\n",
        "tfidf_matrix_compare = tfidf_vectorizer_compare.fit_transform(text_data)\n",
        "\n",
        "# Get the feature names (words)\n",
        "feature_names_compare = tfidf_vectorizer_compare.get_feature_names_out()\n",
        "\n",
        "# Display the TF-IDF matrix for comparison\n",
        "print(\"TF-IDF Matrix (on original data):\")\n",
        "print(tfidf_matrix_compare.toarray())\n",
        "\n",
        "print(\"\\nFeature Names (TF-IDF):\")\n",
        "print(feature_names_compare)\n",
        "\n",
        "print(\"\\nBag-of-Words Matrix (on original data):\")\n",
        "print(bag_of_words_matrix.toarray())\n",
        "\n",
        "print(\"\\nFeature Names (Bag-of-Words):\")\n",
        "print(feature_names)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF Matrix (on original data):\n",
            "[[0.         0.         0.3145322  0.         0.38850984 0.38850984\n",
            "  0.25715068 0.3145322  0.         0.         0.         0.\n",
            "  0.25715068 0.38850984 0.         0.         0.         0.\n",
            "  0.25715068 0.         0.38850984 0.        ]\n",
            " [0.         0.         0.45551258 0.35682424 0.         0.\n",
            "  0.18620569 0.22775629 0.         0.         0.35682424 0.35682424\n",
            "  0.18620569 0.         0.35682424 0.         0.         0.\n",
            "  0.18620569 0.35682424 0.         0.        ]\n",
            " [0.35700721 0.35700721 0.         0.         0.         0.\n",
            "  0.18630117 0.22787308 0.         0.35700721 0.         0.\n",
            "  0.18630117 0.         0.         0.35700721 0.35700721 0.35700721\n",
            "  0.18630117 0.         0.28146859 0.        ]\n",
            " [0.         0.         0.28293955 0.         0.34948664 0.34948664\n",
            "  0.23132162 0.         0.44327948 0.         0.         0.\n",
            "  0.23132162 0.34948664 0.         0.         0.         0.\n",
            "  0.23132162 0.         0.         0.44327948]]\n",
            "\n",
            "Feature Names (TF-IDF):\n",
            "['and' 'and this' 'document' 'document is' 'first' 'first document' 'is'\n",
            " 'is the' 'is this' 'one' 'second' 'second document' 'the' 'the first'\n",
            " 'the second' 'the third' 'third' 'third one' 'this' 'this document'\n",
            " 'this is' 'this the']\n",
            "\n",
            "Bag-of-Words Matrix (on original data):\n",
            "[[0 0 1 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 1 0 1 0]\n",
            " [0 0 2 1 0 0 1 1 0 0 1 1 1 0 1 0 0 0 1 1 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 1 0 0 1 0 0 1 1 1 1 0 1 0]\n",
            " [0 0 1 0 1 1 1 0 1 0 0 0 1 1 0 0 0 0 1 0 0 1]]\n",
            "\n",
            "Feature Names (Bag-of-Words):\n",
            "['and' 'and this' 'document' 'document is' 'first' 'first document' 'is'\n",
            " 'is the' 'is this' 'one' 'second' 'second document' 'the' 'the first'\n",
            " 'the second' 'the third' 'third' 'third one' 'this' 'this document'\n",
            " 'this is' 'this the']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aef25a26"
      },
      "source": [
        "As you can see, the TF-IDF matrix contains floating-point values representing the importance of each term in each document relative to the entire corpus, while the Bag-of-Words matrix contains raw counts of the terms. The feature names should be the same since we used the same `ngram_range`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "269b3a1d"
      },
      "source": [
        "Here are some of the key hyperparameters of `TfidfVectorizer` and their descriptions. Many of these are similar to those in `CountVectorizer`, with some additional ones specific to TF-IDF:\n",
        "\n",
        "*   **`input`**: Type of the input. Can be 'filename', 'file' or 'content'. Default is 'content'.\n",
        "*   **`encoding`**: If bytes or files are given to analyze, this is the encoding used to decode. Default is 'utf-8'.\n",
        "*   **`decode_error`**: Instruction on what to do if a byte sequence is given to analyze that cannot be decoded using `encoding`. 'strict' raises a UnicodeDecodeError. 'ignore' ignores errors. 'replace' replaces errors with a replacement character. Default is 'strict'.\n",
        "*   **`strip_accents`**: Remove accents and perform other character normalization during the preprocessing step. Can be 'ascii', 'unicode', or None. Default is None.\n",
        "*   **`lowercase`**: Convert all characters to lowercase before tokenizing. Default is True.\n",
        "*   **`preprocessor`**: Override the preprocessing (strip_accents and lowercase) stage by providing a callable that takes an entire document as input and returns a transformed document. Default is None.\n",
        "*   **`tokenizer`**: Override the string tokenization step by providing a callable. Default is None (uses the default word tokenizer).\n",
        "*   **`analyzer`**: Whether the feature should be made of word or character n-grams. Can be 'word' or 'char'. Default is 'word'.\n",
        "*   **`stop_words`**: Remove stop words. Can be 'english', a list of strings, or None. Default is None.\n",
        "*   **`token_pattern`**: Regular expression denoting what constitutes a \"token\". Default is `r'(?u)\\b\\w\\w+\\b'`.\n",
        "*   **`ngram_range`**: The lower and upper boundary of the range of n-values for different n-grams to be extracted. All values of n such that `min_n <= n <= max_n` will be used. Default is `(1, 1)` (only unigrams).\n",
        "*   **`max_df`**: When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). Can be a float in the range [0.0, 1.0] or an integer. Default is 1.0.\n",
        "*   **`min_df`**: When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. Can be a float in the range [0.0, 1.0] or an integer. Default is 1.\n",
        "*   **`max_features`**: If not None, build a vocabulary that only considers the top `max_features` ordered by term frequency across the corpus. Default is None.\n",
        "*   **`vocabulary`**: Either a mapping where keys are terms and values are indices in the feature matrix, or an iterable of terms. If not given, a vocabulary is built from the input documents. Default is None.\n",
        "*   **`binary`**: If True, all non-zero counts are set to 1. This is good for discrete probabilistic models that model binary events. Default is False.\n",
        "*   **`dtype`**: Type of the matrix returned by fit_transform() or transform(). Default is `numpy.float64`.\n",
        "*   **`norm`**: Normalization to perform on the tf-idf vectors. Can be 'l1', 'l2' or None. Default is 'l2'.\n",
        "*   **`use_idf`**: Enable inverse-document-frequency reweighting. Default is True.\n",
        "*   **`smooth_idf`**: Smooth idf weights by adding one to document frequencies, as if an extra document was seen containing every term in the collection exactly once. Prevents zero divisions. Default is True.\n",
        "*   **`sublinear_tf`**: Apply sublinear TF scaling, i.e., replace tf with 1 + log(tf). Default is False."
      ]
    }
  ]
}